{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetwork-with-keras.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/01SakuraAyane29/Machine-Learning-Models/blob/master/NeuralNetwork_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLHfkPrvc3-X",
        "colab_type": "code",
        "outputId": "ba3533ec-7b89-4956-9ea9-aff116b70655",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c1a1efa4-3af8-466e-80a3-34670e13e36d\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c1a1efa4-3af8-466e-80a3-34670e13e36d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kc_house_data.csv to kc_house_data.csv\n",
            "User uploaded file \"kc_house_data.csv\" with length 2515206 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP56CFzjcoQu",
        "colab_type": "code",
        "outputId": "e8c372f3-252f-4b48-d31a-120eaf1dd4dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import sklearn as sk\n",
        "import keras\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "data = pd.read_csv('kc_house_data.csv',sep=',')\n",
        "y_data = data['price']\n",
        "x_data = data.drop(columns = ['id','date','price'])\n",
        "\n",
        "date = data['date']\n",
        "\n",
        "date = date.apply(lambda x: time.strptime(x[2:4]+x[4:6]+x[6:8], '%y%m%d'))\n",
        "\n",
        "date = date.apply(lambda x: (time.mktime(datetime.datetime.now().timetuple())\n",
        "                             -time.mktime(x))/86400)\n",
        "\n",
        "x_data.insert(1,'date',date)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_data.dropna(axis = 0)\n",
        "y_data.dropna(axis = 0)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwvHX2VGX9Qk",
        "colab_type": "code",
        "outputId": "3ca7e836-3291-4824-fee6-0ce173319037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "alldata = np.hstack((x_train,y_train.reshape(-1,1)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHwlNwORZTBy",
        "colab_type": "code",
        "outputId": "5024baff-1509-433d-b4fc-1f2364e1ebd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "print(alldata[,0:18])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-6f184fe65847>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print(alldata[,:18])\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5OruDGaxqIG",
        "colab_type": "text"
      },
      "source": [
        "features = Input(shape=(19,))\n",
        "x = Dense(8,activation = 'relu', kernel_initializer = 'glorot_unifrom')(features)\n",
        "price = dense(1,kernel_initializer = 'glorot_uniform')(x)\n",
        "\n",
        "my_model = Model(inputs = [features],outputs = [price])\n",
        "my_model.compile(loss = 'mae',optimizer = 'adam')\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "history = my_model.fit(x_train,y_train, batch_size = BATCH_SIZE, epochs = NUM_EPOCHS, validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS49sPjsdF0A",
        "colab_type": "code",
        "outputId": "108ef015-3d24-4a75-f074-43ddf770e420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Dense\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "y_train = scaler.fit_transform(y_train.reshape(-1,1))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(32,input_shape=(19,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(32))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer = 'sgd',\n",
        "              loss = 'mean_absolute_error')\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "history = model.fit(x_train,y_train,batch_size = BATCH_SIZE,\n",
        "                    epochs = NUM_EPOCHS, validation_split = 0.2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11584 samples, validate on 2896 samples\n",
            "Epoch 1/20\n",
            "11584/11584 [==============================] - 2s 148us/step - loss: 0.4003 - val_loss: 0.3129\n",
            "Epoch 2/20\n",
            "11584/11584 [==============================] - 2s 136us/step - loss: 0.3063 - val_loss: 0.2971\n",
            "Epoch 3/20\n",
            "11584/11584 [==============================] - 2s 132us/step - loss: 0.2896 - val_loss: 0.2836\n",
            "Epoch 4/20\n",
            "11584/11584 [==============================] - 2s 133us/step - loss: 0.2791 - val_loss: 0.2781\n",
            "Epoch 5/20\n",
            "11584/11584 [==============================] - 2s 133us/step - loss: 0.2706 - val_loss: 0.2799\n",
            "Epoch 6/20\n",
            "11584/11584 [==============================] - 2s 133us/step - loss: 0.2616 - val_loss: 0.2646\n",
            "Epoch 7/20\n",
            "11584/11584 [==============================] - 2s 133us/step - loss: 0.2542 - val_loss: 0.2615\n",
            "Epoch 8/20\n",
            "11584/11584 [==============================] - 2s 132us/step - loss: 0.2464 - val_loss: 0.2425\n",
            "Epoch 9/20\n",
            "11584/11584 [==============================] - 2s 134us/step - loss: 0.2376 - val_loss: 0.2402\n",
            "Epoch 10/20\n",
            "11584/11584 [==============================] - 2s 133us/step - loss: 0.2314 - val_loss: 0.2312\n",
            "Epoch 11/20\n",
            "11584/11584 [==============================] - 2s 133us/step - loss: 0.2244 - val_loss: 0.2226\n",
            "Epoch 12/20\n",
            "11584/11584 [==============================] - 2s 133us/step - loss: 0.2206 - val_loss: 0.2210\n",
            "Epoch 13/20\n",
            "11584/11584 [==============================] - 2s 134us/step - loss: 0.2180 - val_loss: 0.2251\n",
            "Epoch 14/20\n",
            "11584/11584 [==============================] - 2s 133us/step - loss: 0.2156 - val_loss: 0.2189\n",
            "Epoch 15/20\n",
            "11584/11584 [==============================] - 2s 133us/step - loss: 0.2128 - val_loss: 0.2114\n",
            "Epoch 16/20\n",
            "11584/11584 [==============================] - 2s 133us/step - loss: 0.2107 - val_loss: 0.2126\n",
            "Epoch 17/20\n",
            "11584/11584 [==============================] - 2s 133us/step - loss: 0.2098 - val_loss: 0.2335\n",
            "Epoch 18/20\n",
            "11584/11584 [==============================] - 2s 132us/step - loss: 0.2074 - val_loss: 0.2091\n",
            "Epoch 19/20\n",
            "11584/11584 [==============================] - 2s 133us/step - loss: 0.2055 - val_loss: 0.2082\n",
            "Epoch 20/20\n",
            "11584/11584 [==============================] - 2s 132us/step - loss: 0.2043 - val_loss: 0.2242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6NVrZPj2M1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Dense\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "maxscaler = MaxAbsScaler()\n",
        "\n",
        "x_train = maxscaler.fit_transform(x_train)\n",
        "\n",
        "mymodel = Sequential()\n",
        "mymodel.add(Dense(8,activation = 'relu',input_shape=(19,)))\n",
        "mymodel.add(Dense(16))\n",
        "mymodel.add(Dense(32,activation = 'relu'))\n",
        "mymodel.add(Dense(64))\n",
        "mymodel.add(Dense(128,activation = 'relu'))\n",
        "mymodel.add(Dense(256,activation = 'sigmoid'))\n",
        "mymodel.add(Dense(128))\n",
        "mymodel.add(Dense(64))\n",
        "mymodel.add(Dense(32))\n",
        "mymodel.add(Dense(1))\n",
        "mymodel.compile(optimizer = 'adam',\n",
        "              loss = 'mean_absolute_error')\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "history = mymodel.fit(x_train,y_train,batch_size = BATCH_SIZE,\n",
        "                    epochs = NUM_EPOCHS, validation_split = 0.2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdCiBUDtiuhF",
        "colab_type": "code",
        "outputId": "0082357f-c865-4473-a9c7-477b4ff9c4b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3434
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Dense\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "maxscaler = MinMaxScaler()\n",
        "\n",
        "x_train = maxscaler.fit_transform(x_train)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(8,activation = 'relu',input_shape=(19,)))\n",
        "model.add(Dense(16))\n",
        "model.add(Dense(32,activation = 'relu'))\n",
        "model.add(Dense(64))\n",
        "model.add(Dense(128,activation = 'relu'))\n",
        "model.add(Dense(256,activation = 'sigmoid'))\n",
        "model.add(Dense(128))\n",
        "model.add(Dense(64))\n",
        "model.add(Dense(32))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'mean_absolute_error')\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "history = model.fit(x_train,y_train,batch_size = BATCH_SIZE,\n",
        "                    epochs = NUM_EPOCHS, validation_split = 0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11584 samples, validate on 2896 samples\n",
            "Epoch 1/100\n",
            "11584/11584 [==============================] - 3s 288us/step - loss: 182420.1691 - val_loss: 111669.1019\n",
            "Epoch 2/100\n",
            "11584/11584 [==============================] - 3s 249us/step - loss: 108187.0429 - val_loss: 102416.9765\n",
            "Epoch 3/100\n",
            "11584/11584 [==============================] - 3s 249us/step - loss: 98248.8274 - val_loss: 89401.7307\n",
            "Epoch 4/100\n",
            "11584/11584 [==============================] - 3s 252us/step - loss: 92345.1232 - val_loss: 85182.4573\n",
            "Epoch 5/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 88183.5935 - val_loss: 82935.1044\n",
            "Epoch 6/100\n",
            "11584/11584 [==============================] - 3s 249us/step - loss: 86062.5400 - val_loss: 85172.3528\n",
            "Epoch 7/100\n",
            "11584/11584 [==============================] - 3s 248us/step - loss: 85088.2184 - val_loss: 79941.3913\n",
            "Epoch 8/100\n",
            "11584/11584 [==============================] - 3s 245us/step - loss: 83353.7246 - val_loss: 77563.7721\n",
            "Epoch 9/100\n",
            "11584/11584 [==============================] - 3s 244us/step - loss: 82316.6965 - val_loss: 81570.1487\n",
            "Epoch 10/100\n",
            "11584/11584 [==============================] - 3s 245us/step - loss: 81719.9100 - val_loss: 78963.7048\n",
            "Epoch 11/100\n",
            "11584/11584 [==============================] - 3s 245us/step - loss: 80877.5920 - val_loss: 78202.9008\n",
            "Epoch 12/100\n",
            "11584/11584 [==============================] - 3s 246us/step - loss: 80264.6988 - val_loss: 78981.8998\n",
            "Epoch 13/100\n",
            "11584/11584 [==============================] - 3s 246us/step - loss: 81423.2668 - val_loss: 78307.8597\n",
            "Epoch 14/100\n",
            "11584/11584 [==============================] - 3s 244us/step - loss: 80298.7120 - val_loss: 79860.3942\n",
            "Epoch 15/100\n",
            "11584/11584 [==============================] - 3s 252us/step - loss: 78554.6828 - val_loss: 80486.0170\n",
            "Epoch 16/100\n",
            "11584/11584 [==============================] - 3s 249us/step - loss: 78819.3620 - val_loss: 79683.5192\n",
            "Epoch 17/100\n",
            "11584/11584 [==============================] - 3s 248us/step - loss: 77761.0852 - val_loss: 74447.9396\n",
            "Epoch 18/100\n",
            "11584/11584 [==============================] - 3s 245us/step - loss: 77739.5706 - val_loss: 73420.2620\n",
            "Epoch 19/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 76790.8122 - val_loss: 77003.0660\n",
            "Epoch 20/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 77054.2055 - val_loss: 80019.7073\n",
            "Epoch 21/100\n",
            "11584/11584 [==============================] - 3s 248us/step - loss: 76887.9465 - val_loss: 74874.3905\n",
            "Epoch 22/100\n",
            "11584/11584 [==============================] - 3s 240us/step - loss: 75456.0011 - val_loss: 72472.0011\n",
            "Epoch 23/100\n",
            "11584/11584 [==============================] - 3s 241us/step - loss: 76409.0669 - val_loss: 101082.1522\n",
            "Epoch 24/100\n",
            "11584/11584 [==============================] - 3s 242us/step - loss: 75131.1519 - val_loss: 74898.4562\n",
            "Epoch 25/100\n",
            "11584/11584 [==============================] - 3s 242us/step - loss: 75920.7337 - val_loss: 74897.3152\n",
            "Epoch 26/100\n",
            "11584/11584 [==============================] - 3s 253us/step - loss: 75019.8575 - val_loss: 72646.5267\n",
            "Epoch 27/100\n",
            "11584/11584 [==============================] - 3s 245us/step - loss: 75021.0686 - val_loss: 80577.8074\n",
            "Epoch 28/100\n",
            "11584/11584 [==============================] - 3s 240us/step - loss: 74796.3070 - val_loss: 73351.9118\n",
            "Epoch 29/100\n",
            "11584/11584 [==============================] - 3s 240us/step - loss: 75085.5784 - val_loss: 75103.3734\n",
            "Epoch 30/100\n",
            "11584/11584 [==============================] - 3s 244us/step - loss: 75438.1610 - val_loss: 72741.8345\n",
            "Epoch 31/100\n",
            "11584/11584 [==============================] - 3s 253us/step - loss: 74286.2193 - val_loss: 74548.2781\n",
            "Epoch 32/100\n",
            "11584/11584 [==============================] - 3s 250us/step - loss: 73367.0226 - val_loss: 75763.9020\n",
            "Epoch 33/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 74090.7439 - val_loss: 72459.5248\n",
            "Epoch 34/100\n",
            "11584/11584 [==============================] - 3s 251us/step - loss: 74129.8368 - val_loss: 72337.1435\n",
            "Epoch 35/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 74405.6796 - val_loss: 73426.5335\n",
            "Epoch 36/100\n",
            "11584/11584 [==============================] - 3s 253us/step - loss: 73586.1408 - val_loss: 69114.4553\n",
            "Epoch 37/100\n",
            "11584/11584 [==============================] - 3s 249us/step - loss: 73232.5571 - val_loss: 71163.6724\n",
            "Epoch 38/100\n",
            "11584/11584 [==============================] - 3s 250us/step - loss: 73592.9666 - val_loss: 70503.2653\n",
            "Epoch 39/100\n",
            "11584/11584 [==============================] - 3s 251us/step - loss: 73028.9044 - val_loss: 72101.8278\n",
            "Epoch 40/100\n",
            "11584/11584 [==============================] - 3s 249us/step - loss: 72360.1793 - val_loss: 73697.9765\n",
            "Epoch 41/100\n",
            "11584/11584 [==============================] - 3s 249us/step - loss: 73841.3892 - val_loss: 72287.6501\n",
            "Epoch 42/100\n",
            "11584/11584 [==============================] - 3s 246us/step - loss: 73228.6700 - val_loss: 74399.5731\n",
            "Epoch 43/100\n",
            "11584/11584 [==============================] - 3s 242us/step - loss: 73020.5746 - val_loss: 75807.3546\n",
            "Epoch 44/100\n",
            "11584/11584 [==============================] - 3s 244us/step - loss: 72261.6990 - val_loss: 71795.5149\n",
            "Epoch 45/100\n",
            "11584/11584 [==============================] - 3s 248us/step - loss: 72533.8732 - val_loss: 72283.5650\n",
            "Epoch 46/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 72739.6115 - val_loss: 72400.5106\n",
            "Epoch 47/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 72480.9404 - val_loss: 76013.3034\n",
            "Epoch 48/100\n",
            "11584/11584 [==============================] - 3s 246us/step - loss: 73082.1327 - val_loss: 85139.1955\n",
            "Epoch 49/100\n",
            "11584/11584 [==============================] - 3s 244us/step - loss: 71529.8096 - val_loss: 77415.2346\n",
            "Epoch 50/100\n",
            "11584/11584 [==============================] - 3s 249us/step - loss: 72263.2790 - val_loss: 73528.0112\n",
            "Epoch 51/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 71599.7568 - val_loss: 71577.2407\n",
            "Epoch 52/100\n",
            "11584/11584 [==============================] - 3s 242us/step - loss: 71963.1705 - val_loss: 73333.9658\n",
            "Epoch 53/100\n",
            "11584/11584 [==============================] - 3s 246us/step - loss: 72137.5618 - val_loss: 78782.8299\n",
            "Epoch 54/100\n",
            "11584/11584 [==============================] - 3s 250us/step - loss: 71869.8160 - val_loss: 74345.0138\n",
            "Epoch 55/100\n",
            "11584/11584 [==============================] - 3s 244us/step - loss: 72585.3661 - val_loss: 71170.7429\n",
            "Epoch 56/100\n",
            "11584/11584 [==============================] - 3s 245us/step - loss: 71233.1259 - val_loss: 72235.9940\n",
            "Epoch 57/100\n",
            "11584/11584 [==============================] - 3s 246us/step - loss: 72095.7552 - val_loss: 77486.7921\n",
            "Epoch 58/100\n",
            "11584/11584 [==============================] - 3s 245us/step - loss: 71605.7275 - val_loss: 68799.9891\n",
            "Epoch 59/100\n",
            "11584/11584 [==============================] - 3s 242us/step - loss: 71260.2478 - val_loss: 72278.0282\n",
            "Epoch 60/100\n",
            "11584/11584 [==============================] - 3s 244us/step - loss: 71138.1204 - val_loss: 71172.9024\n",
            "Epoch 61/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 71027.9654 - val_loss: 74469.9515\n",
            "Epoch 62/100\n",
            "11584/11584 [==============================] - 3s 260us/step - loss: 70911.0985 - val_loss: 70183.8291\n",
            "Epoch 63/100\n",
            "11584/11584 [==============================] - 3s 269us/step - loss: 71357.6912 - val_loss: 71350.8032\n",
            "Epoch 64/100\n",
            "11584/11584 [==============================] - 3s 267us/step - loss: 71470.0505 - val_loss: 73470.0352\n",
            "Epoch 65/100\n",
            "11584/11584 [==============================] - 3s 260us/step - loss: 70911.6504 - val_loss: 69413.2169\n",
            "Epoch 66/100\n",
            "11584/11584 [==============================] - 3s 259us/step - loss: 70097.3612 - val_loss: 70145.7112\n",
            "Epoch 67/100\n",
            "11584/11584 [==============================] - 3s 270us/step - loss: 70889.9733 - val_loss: 76860.1890\n",
            "Epoch 68/100\n",
            "11584/11584 [==============================] - 3s 252us/step - loss: 70625.1281 - val_loss: 73798.6218\n",
            "Epoch 69/100\n",
            "11584/11584 [==============================] - 3s 249us/step - loss: 70164.2543 - val_loss: 70181.5879\n",
            "Epoch 70/100\n",
            "11584/11584 [==============================] - 3s 249us/step - loss: 70121.7927 - val_loss: 73749.1099\n",
            "Epoch 71/100\n",
            "11584/11584 [==============================] - 3s 248us/step - loss: 70581.3166 - val_loss: 71932.2508\n",
            "Epoch 72/100\n",
            "11584/11584 [==============================] - 3s 248us/step - loss: 70147.7873 - val_loss: 76806.7516\n",
            "Epoch 73/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 70402.4444 - val_loss: 67910.1593\n",
            "Epoch 74/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 69824.6874 - val_loss: 69931.6528\n",
            "Epoch 75/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 70638.1869 - val_loss: 69314.3727\n",
            "Epoch 76/100\n",
            "11584/11584 [==============================] - 3s 253us/step - loss: 70438.4303 - val_loss: 72119.7892\n",
            "Epoch 77/100\n",
            "11584/11584 [==============================] - 3s 249us/step - loss: 70676.5721 - val_loss: 70936.1923\n",
            "Epoch 78/100\n",
            "11584/11584 [==============================] - 3s 254us/step - loss: 70033.0359 - val_loss: 69003.3968\n",
            "Epoch 79/100\n",
            "11584/11584 [==============================] - 3s 253us/step - loss: 68856.8209 - val_loss: 69587.1616\n",
            "Epoch 80/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 69903.5764 - val_loss: 68688.6073\n",
            "Epoch 81/100\n",
            "11584/11584 [==============================] - 3s 245us/step - loss: 70041.4307 - val_loss: 68841.8425\n",
            "Epoch 82/100\n",
            "11584/11584 [==============================] - 3s 246us/step - loss: 68807.1235 - val_loss: 74109.7092\n",
            "Epoch 83/100\n",
            "11584/11584 [==============================] - 3s 252us/step - loss: 69283.7686 - val_loss: 68987.9172\n",
            "Epoch 84/100\n",
            "11584/11584 [==============================] - 3s 251us/step - loss: 68784.6546 - val_loss: 70578.7042\n",
            "Epoch 85/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 70986.1013 - val_loss: 72776.5897\n",
            "Epoch 86/100\n",
            "11584/11584 [==============================] - 3s 241us/step - loss: 69737.7281 - val_loss: 74387.3009\n",
            "Epoch 87/100\n",
            "11584/11584 [==============================] - 3s 240us/step - loss: 69334.8773 - val_loss: 71017.9007\n",
            "Epoch 88/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 68920.8911 - val_loss: 67825.0764\n",
            "Epoch 89/100\n",
            "11584/11584 [==============================] - 3s 248us/step - loss: 68729.2620 - val_loss: 69451.6681\n",
            "Epoch 90/100\n",
            "11584/11584 [==============================] - 3s 246us/step - loss: 68281.7155 - val_loss: 72792.6013\n",
            "Epoch 91/100\n",
            "11584/11584 [==============================] - 3s 245us/step - loss: 68629.0105 - val_loss: 74666.4207\n",
            "Epoch 92/100\n",
            "11584/11584 [==============================] - 3s 245us/step - loss: 68604.5729 - val_loss: 71468.6417\n",
            "Epoch 93/100\n",
            "11584/11584 [==============================] - 3s 249us/step - loss: 69181.1505 - val_loss: 73819.8847\n",
            "Epoch 94/100\n",
            "11584/11584 [==============================] - 3s 245us/step - loss: 68893.2665 - val_loss: 76412.1710\n",
            "Epoch 95/100\n",
            "11584/11584 [==============================] - 3s 248us/step - loss: 68476.8714 - val_loss: 69868.8824\n",
            "Epoch 96/100\n",
            "11584/11584 [==============================] - 3s 246us/step - loss: 67905.2490 - val_loss: 69025.0168\n",
            "Epoch 97/100\n",
            "11584/11584 [==============================] - 3s 248us/step - loss: 68272.0096 - val_loss: 71175.3712\n",
            "Epoch 98/100\n",
            "11584/11584 [==============================] - 3s 246us/step - loss: 68395.1814 - val_loss: 68916.4574\n",
            "Epoch 99/100\n",
            "11584/11584 [==============================] - 3s 246us/step - loss: 68516.7014 - val_loss: 78700.4655\n",
            "Epoch 100/100\n",
            "11584/11584 [==============================] - 3s 251us/step - loss: 69000.9882 - val_loss: 71259.7826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLGfqeValpyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Dense\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "maxscaler = MinMaxScaler()\n",
        "\n",
        "x_train = maxscaler.fit_transform(x_train)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(8,activation = 'relu',input_shape=(19,)))\n",
        "model.add(Dense(16))\n",
        "model.add(Dense(32,activation = 'relu'))\n",
        "model.add(Dense(64))\n",
        "model.add(Dense(128,activation = 'relu'))\n",
        "model.add(Dense(64))\n",
        "model.add(Dense(32))\n",
        "model.add(Dense(1,activation = 'linear'))\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'mean_absolute_error')\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "history = model.fit(x_train,y_train,batch_size = BATCH_SIZE,\n",
        "                    epochs = NUM_EPOCHS, validation_split = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fohEyyHnIDSh",
        "colab_type": "code",
        "outputId": "2aa5705a-a531-4ce0-b598-b2c4b842bbe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3692
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Dense\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "maxscaler = MinMaxScaler()\n",
        "\n",
        "x_train = maxscaler.fit_transform(x_train)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(8,activation = 'relu',input_shape=(19,)))\n",
        "model.add(Dense(16))\n",
        "model.add(Dense(32,activation = 'relu'))\n",
        "model.add(Dense(64))\n",
        "model.add(Dense(128,activation = 'relu'))\n",
        "model.add(Dense(256,activation = 'sigmoid'))\n",
        "model.add(Dense(128))\n",
        "model.add(Dense(64))\n",
        "model.add(Dense(32))\n",
        "model.add(Dense(1, activation = 'linear'))\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'mean_absolute_error')\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "history = model.fit(x_train,y_train,batch_size = BATCH_SIZE,\n",
        "                    epochs = NUM_EPOCHS, validation_split = 0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11584 samples, validate on 2896 samples\n",
            "Epoch 1/100\n",
            "11584/11584 [==============================] - 3s 291us/step - loss: 189087.1847 - val_loss: 116383.5341\n",
            "Epoch 2/100\n",
            "11584/11584 [==============================] - 3s 275us/step - loss: 110290.2091 - val_loss: 103392.6463\n",
            "Epoch 3/100\n",
            "11584/11584 [==============================] - 3s 276us/step - loss: 98596.1801 - val_loss: 102984.3414\n",
            "Epoch 4/100\n",
            "11584/11584 [==============================] - 3s 275us/step - loss: 93569.4483 - val_loss: 87201.1764\n",
            "Epoch 5/100\n",
            "11584/11584 [==============================] - 3s 255us/step - loss: 91862.7916 - val_loss: 82933.8848\n",
            "Epoch 6/100\n",
            "11584/11584 [==============================] - 3s 257us/step - loss: 88517.6022 - val_loss: 95967.5731\n",
            "Epoch 7/100\n",
            "11584/11584 [==============================] - 3s 275us/step - loss: 87065.2470 - val_loss: 85361.2215\n",
            "Epoch 8/100\n",
            "11584/11584 [==============================] - 3s 282us/step - loss: 85502.6909 - val_loss: 85568.7910\n",
            "Epoch 9/100\n",
            "11584/11584 [==============================] - 3s 283us/step - loss: 85485.7838 - val_loss: 85974.7830\n",
            "Epoch 10/100\n",
            "11584/11584 [==============================] - 3s 285us/step - loss: 83849.7814 - val_loss: 84499.9589\n",
            "Epoch 11/100\n",
            "11584/11584 [==============================] - 3s 285us/step - loss: 83410.9141 - val_loss: 83734.5051\n",
            "Epoch 12/100\n",
            "11584/11584 [==============================] - 3s 285us/step - loss: 81580.4832 - val_loss: 77269.8301\n",
            "Epoch 13/100\n",
            "11584/11584 [==============================] - 3s 283us/step - loss: 81844.5237 - val_loss: 81827.8867\n",
            "Epoch 14/100\n",
            "11584/11584 [==============================] - 3s 285us/step - loss: 79493.0085 - val_loss: 75716.8229\n",
            "Epoch 15/100\n",
            "11584/11584 [==============================] - 3s 286us/step - loss: 79416.3604 - val_loss: 73218.1216\n",
            "Epoch 16/100\n",
            "11584/11584 [==============================] - 3s 284us/step - loss: 78620.8499 - val_loss: 76507.1529\n",
            "Epoch 17/100\n",
            "11584/11584 [==============================] - 3s 279us/step - loss: 77973.2473 - val_loss: 74954.0259\n",
            "Epoch 18/100\n",
            "11584/11584 [==============================] - 3s 276us/step - loss: 77044.8665 - val_loss: 77805.9933\n",
            "Epoch 19/100\n",
            "11584/11584 [==============================] - 3s 281us/step - loss: 77317.3168 - val_loss: 78796.0789\n",
            "Epoch 20/100\n",
            "11584/11584 [==============================] - 3s 284us/step - loss: 76975.2906 - val_loss: 81631.4883\n",
            "Epoch 21/100\n",
            "11584/11584 [==============================] - 3s 292us/step - loss: 76268.5260 - val_loss: 77654.6787\n",
            "Epoch 22/100\n",
            "11584/11584 [==============================] - 3s 287us/step - loss: 75931.2666 - val_loss: 74220.4934\n",
            "Epoch 23/100\n",
            "11584/11584 [==============================] - 3s 286us/step - loss: 75553.8746 - val_loss: 80393.0540\n",
            "Epoch 24/100\n",
            "11584/11584 [==============================] - 3s 284us/step - loss: 76395.1949 - val_loss: 75401.7795\n",
            "Epoch 25/100\n",
            "11584/11584 [==============================] - 3s 286us/step - loss: 76167.1249 - val_loss: 76638.2853\n",
            "Epoch 26/100\n",
            "11584/11584 [==============================] - 3s 290us/step - loss: 75257.2244 - val_loss: 78743.2765\n",
            "Epoch 27/100\n",
            "11584/11584 [==============================] - 3s 289us/step - loss: 75184.5841 - val_loss: 77409.5169\n",
            "Epoch 28/100\n",
            "11584/11584 [==============================] - 3s 286us/step - loss: 74493.3672 - val_loss: 70741.3094\n",
            "Epoch 29/100\n",
            "11584/11584 [==============================] - 3s 280us/step - loss: 74314.1214 - val_loss: 71742.3724\n",
            "Epoch 30/100\n",
            "11584/11584 [==============================] - 3s 283us/step - loss: 73960.2546 - val_loss: 74090.0093\n",
            "Epoch 31/100\n",
            "11584/11584 [==============================] - 3s 281us/step - loss: 73436.7532 - val_loss: 70972.2566\n",
            "Epoch 32/100\n",
            "11584/11584 [==============================] - 3s 285us/step - loss: 73863.0590 - val_loss: 70790.3335\n",
            "Epoch 33/100\n",
            "11584/11584 [==============================] - 3s 284us/step - loss: 73863.9354 - val_loss: 78419.4718\n",
            "Epoch 34/100\n",
            "11584/11584 [==============================] - 3s 284us/step - loss: 74300.0648 - val_loss: 74156.2888\n",
            "Epoch 35/100\n",
            "11584/11584 [==============================] - 3s 281us/step - loss: 73122.8311 - val_loss: 73487.4832\n",
            "Epoch 36/100\n",
            "11584/11584 [==============================] - 3s 279us/step - loss: 74614.9517 - val_loss: 74056.8558\n",
            "Epoch 37/100\n",
            "11584/11584 [==============================] - 3s 282us/step - loss: 72754.0362 - val_loss: 69670.4268\n",
            "Epoch 38/100\n",
            "11584/11584 [==============================] - 3s 287us/step - loss: 73140.6849 - val_loss: 74948.4070\n",
            "Epoch 39/100\n",
            "11584/11584 [==============================] - 3s 295us/step - loss: 72617.0107 - val_loss: 79667.6330\n",
            "Epoch 40/100\n",
            "11584/11584 [==============================] - 3s 291us/step - loss: 73708.2074 - val_loss: 76546.1740\n",
            "Epoch 41/100\n",
            "11584/11584 [==============================] - 3s 302us/step - loss: 71692.6047 - val_loss: 76598.7984\n",
            "Epoch 42/100\n",
            "11584/11584 [==============================] - 3s 291us/step - loss: 72050.9785 - val_loss: 68916.7317\n",
            "Epoch 43/100\n",
            "11584/11584 [==============================] - 3s 281us/step - loss: 72844.5780 - val_loss: 69820.4956\n",
            "Epoch 44/100\n",
            "11584/11584 [==============================] - 3s 285us/step - loss: 72171.0962 - val_loss: 73853.2617\n",
            "Epoch 45/100\n",
            "11584/11584 [==============================] - 3s 287us/step - loss: 71407.8838 - val_loss: 71184.9076\n",
            "Epoch 46/100\n",
            "11584/11584 [==============================] - 3s 286us/step - loss: 71564.0164 - val_loss: 72365.9350\n",
            "Epoch 47/100\n",
            "11584/11584 [==============================] - 3s 286us/step - loss: 71654.2312 - val_loss: 72705.6962\n",
            "Epoch 48/100\n",
            "11584/11584 [==============================] - 3s 292us/step - loss: 71631.4065 - val_loss: 73762.0888\n",
            "Epoch 49/100\n",
            "11584/11584 [==============================] - 3s 288us/step - loss: 71537.1778 - val_loss: 70332.9829\n",
            "Epoch 50/100\n",
            "11584/11584 [==============================] - 3s 289us/step - loss: 72751.7650 - val_loss: 69673.6273\n",
            "Epoch 51/100\n",
            "11584/11584 [==============================] - 3s 287us/step - loss: 71192.7685 - val_loss: 72682.4115\n",
            "Epoch 52/100\n",
            "11584/11584 [==============================] - 3s 286us/step - loss: 70720.1443 - val_loss: 70726.9291\n",
            "Epoch 53/100\n",
            "11584/11584 [==============================] - 3s 289us/step - loss: 70845.1004 - val_loss: 70343.0295\n",
            "Epoch 54/100\n",
            "11584/11584 [==============================] - 3s 291us/step - loss: 71197.7592 - val_loss: 75076.8577\n",
            "Epoch 55/100\n",
            "11584/11584 [==============================] - 3s 290us/step - loss: 69800.3440 - val_loss: 72980.9034\n",
            "Epoch 56/100\n",
            "11584/11584 [==============================] - 3s 290us/step - loss: 70708.4607 - val_loss: 71893.8361\n",
            "Epoch 57/100\n",
            "11584/11584 [==============================] - 3s 297us/step - loss: 71118.2512 - val_loss: 71267.1390\n",
            "Epoch 58/100\n",
            "11584/11584 [==============================] - 3s 287us/step - loss: 70618.5008 - val_loss: 71112.0548\n",
            "Epoch 59/100\n",
            "11584/11584 [==============================] - 3s 285us/step - loss: 71500.7979 - val_loss: 72742.9601\n",
            "Epoch 60/100\n",
            "11584/11584 [==============================] - 3s 280us/step - loss: 70485.9104 - val_loss: 72676.4441\n",
            "Epoch 61/100\n",
            "11584/11584 [==============================] - 3s 281us/step - loss: 69894.7834 - val_loss: 68837.3003\n",
            "Epoch 62/100\n",
            "11584/11584 [==============================] - 3s 289us/step - loss: 70481.2854 - val_loss: 68777.9295\n",
            "Epoch 63/100\n",
            "11584/11584 [==============================] - 3s 284us/step - loss: 70396.6313 - val_loss: 69854.5494\n",
            "Epoch 64/100\n",
            "11584/11584 [==============================] - 3s 284us/step - loss: 69739.4332 - val_loss: 73973.0628\n",
            "Epoch 65/100\n",
            "11584/11584 [==============================] - 3s 282us/step - loss: 68732.0927 - val_loss: 71632.3994\n",
            "Epoch 66/100\n",
            "11584/11584 [==============================] - 3s 281us/step - loss: 68408.8316 - val_loss: 73239.8798\n",
            "Epoch 67/100\n",
            "11584/11584 [==============================] - 3s 281us/step - loss: 69806.8099 - val_loss: 75502.8530\n",
            "Epoch 68/100\n",
            "11584/11584 [==============================] - 3s 282us/step - loss: 68515.9212 - val_loss: 70454.3595\n",
            "Epoch 69/100\n",
            "11584/11584 [==============================] - 3s 279us/step - loss: 69315.2088 - val_loss: 78543.7650\n",
            "Epoch 70/100\n",
            "11584/11584 [==============================] - 3s 282us/step - loss: 69032.5079 - val_loss: 69170.7099\n",
            "Epoch 71/100\n",
            "11584/11584 [==============================] - 3s 280us/step - loss: 69796.6800 - val_loss: 67721.2446\n",
            "Epoch 72/100\n",
            "11584/11584 [==============================] - 3s 286us/step - loss: 68388.6339 - val_loss: 70220.2612\n",
            "Epoch 73/100\n",
            "11584/11584 [==============================] - 3s 283us/step - loss: 68909.4633 - val_loss: 72819.3565\n",
            "Epoch 74/100\n",
            "11584/11584 [==============================] - 3s 284us/step - loss: 68273.6401 - val_loss: 69327.7459\n",
            "Epoch 75/100\n",
            "11584/11584 [==============================] - 3s 284us/step - loss: 68410.1819 - val_loss: 69804.3371\n",
            "Epoch 76/100\n",
            "11584/11584 [==============================] - 3s 284us/step - loss: 68337.5589 - val_loss: 70903.6634\n",
            "Epoch 77/100\n",
            "11584/11584 [==============================] - 3s 282us/step - loss: 68088.4794 - val_loss: 71492.3861\n",
            "Epoch 78/100\n",
            "11584/11584 [==============================] - 3s 282us/step - loss: 68107.0521 - val_loss: 69302.4649\n",
            "Epoch 79/100\n",
            "11584/11584 [==============================] - 3s 283us/step - loss: 68193.4111 - val_loss: 72978.2229\n",
            "Epoch 80/100\n",
            "11584/11584 [==============================] - 3s 284us/step - loss: 68913.0555 - val_loss: 72137.7232\n",
            "Epoch 81/100\n",
            "11584/11584 [==============================] - 3s 285us/step - loss: 68665.0506 - val_loss: 69330.2935\n",
            "Epoch 82/100\n",
            "11584/11584 [==============================] - 3s 284us/step - loss: 67317.8058 - val_loss: 70749.1151\n",
            "Epoch 83/100\n",
            "11584/11584 [==============================] - 3s 290us/step - loss: 68152.2396 - val_loss: 69589.4811\n",
            "Epoch 84/100\n",
            "11584/11584 [==============================] - 3s 282us/step - loss: 67593.9123 - val_loss: 74434.2628\n",
            "Epoch 85/100\n",
            "11584/11584 [==============================] - 3s 286us/step - loss: 67957.9070 - val_loss: 68737.2283\n",
            "Epoch 86/100\n",
            "11584/11584 [==============================] - 3s 284us/step - loss: 67460.0246 - val_loss: 68576.3296\n",
            "Epoch 87/100\n",
            "11584/11584 [==============================] - 3s 288us/step - loss: 66795.9516 - val_loss: 69073.4515\n",
            "Epoch 88/100\n",
            "11584/11584 [==============================] - 3s 284us/step - loss: 67522.5613 - val_loss: 71136.9275\n",
            "Epoch 89/100\n",
            "11584/11584 [==============================] - 3s 282us/step - loss: 68110.7143 - val_loss: 68509.2622\n",
            "Epoch 90/100\n",
            "11584/11584 [==============================] - 3s 290us/step - loss: 67284.7697 - val_loss: 70855.1546\n",
            "Epoch 91/100\n",
            "11584/11584 [==============================] - 3s 289us/step - loss: 66806.0957 - val_loss: 73602.4477\n",
            "Epoch 92/100\n",
            "11584/11584 [==============================] - 3s 289us/step - loss: 67382.2150 - val_loss: 68843.8472\n",
            "Epoch 93/100\n",
            "11584/11584 [==============================] - 3s 290us/step - loss: 67045.3636 - val_loss: 67545.5599\n",
            "Epoch 94/100\n",
            "11584/11584 [==============================] - 4s 315us/step - loss: 66392.5894 - val_loss: 68047.2682\n",
            "Epoch 95/100\n",
            "11584/11584 [==============================] - 4s 320us/step - loss: 67148.2595 - val_loss: 74051.5002\n",
            "Epoch 96/100\n",
            "11584/11584 [==============================] - 4s 321us/step - loss: 67028.1463 - val_loss: 70702.0543\n",
            "Epoch 97/100\n",
            "11584/11584 [==============================] - 4s 327us/step - loss: 67261.2586 - val_loss: 70146.5883\n",
            "Epoch 98/100\n",
            "11584/11584 [==============================] - 4s 310us/step - loss: 66504.0652 - val_loss: 69915.5259\n",
            "Epoch 99/100\n",
            "11584/11584 [==============================] - 3s 293us/step - loss: 66420.4363 - val_loss: 70075.4549\n",
            "Epoch 100/100\n",
            "11584/11584 [==============================] - 3s 295us/step - loss: 67143.1161 - val_loss: 71951.3354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wstndrBSESN5",
        "colab_type": "code",
        "outputId": "59ee486f-a3ac-433a-d433-46e24aa94c1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'],loc = 'upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAFnCAYAAAA7VkqGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fc090392050>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9JmoQwVJuFI",
        "colab_type": "code",
        "outputId": "1898f83f-f6c8-4bc2-d705-becd93a63f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3434
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Dense\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "maxscaler = MinMaxScaler()\n",
        "\n",
        "x_train = maxscaler.fit_transform(x_train)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(8,activation = 'relu',input_shape=(19,)))\n",
        "model.add(Dense(16))\n",
        "model.add(Dense(32,activation = 'relu'))\n",
        "model.add(Dense(64))\n",
        "model.add(Dense(128,activation = 'relu'))\n",
        "model.add(Dense(256))\n",
        "model.add(Dense(512))\n",
        "model.add(Dense(128))\n",
        "model.add(Dense(64))\n",
        "model.add(Dense(32))\n",
        "model.add(Dense(1, activation = 'linear'))\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'mean_absolute_error')\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "history = model.fit(x_train,y_train,batch_size = BATCH_SIZE,\n",
        "                    epochs = NUM_EPOCHS, validation_split = 0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11584 samples, validate on 2896 samples\n",
            "Epoch 1/100\n",
            "11584/11584 [==============================] - 9s 753us/step - loss: 157927.8006 - val_loss: 120422.9020\n",
            "Epoch 2/100\n",
            "11584/11584 [==============================] - 8s 701us/step - loss: 117477.6776 - val_loss: 115095.2264\n",
            "Epoch 3/100\n",
            "11584/11584 [==============================] - 8s 700us/step - loss: 111837.7030 - val_loss: 106992.0556\n",
            "Epoch 4/100\n",
            "11584/11584 [==============================] - 8s 721us/step - loss: 108671.5538 - val_loss: 106288.5444\n",
            "Epoch 5/100\n",
            "11584/11584 [==============================] - 8s 722us/step - loss: 104957.2428 - val_loss: 101551.5142\n",
            "Epoch 6/100\n",
            "11584/11584 [==============================] - 9s 754us/step - loss: 102204.3440 - val_loss: 98738.0817\n",
            "Epoch 7/100\n",
            "11584/11584 [==============================] - 9s 753us/step - loss: 100381.0351 - val_loss: 112372.5894\n",
            "Epoch 8/100\n",
            "11584/11584 [==============================] - 9s 761us/step - loss: 100402.1424 - val_loss: 98482.2964\n",
            "Epoch 9/100\n",
            "11584/11584 [==============================] - 9s 764us/step - loss: 94433.6492 - val_loss: 89117.9887\n",
            "Epoch 10/100\n",
            "11584/11584 [==============================] - 9s 743us/step - loss: 93475.6553 - val_loss: 86611.3091\n",
            "Epoch 11/100\n",
            "11584/11584 [==============================] - 9s 749us/step - loss: 90072.9174 - val_loss: 84110.8043\n",
            "Epoch 12/100\n",
            "11584/11584 [==============================] - 9s 742us/step - loss: 89689.6076 - val_loss: 81763.8658\n",
            "Epoch 13/100\n",
            "11584/11584 [==============================] - 9s 778us/step - loss: 87802.5499 - val_loss: 89856.6589\n",
            "Epoch 14/100\n",
            "11584/11584 [==============================] - 10s 828us/step - loss: 85312.9330 - val_loss: 88055.7659\n",
            "Epoch 15/100\n",
            "11584/11584 [==============================] - 10s 828us/step - loss: 84530.9147 - val_loss: 82237.2274\n",
            "Epoch 16/100\n",
            "11584/11584 [==============================] - 8s 727us/step - loss: 83691.1545 - val_loss: 78637.4159\n",
            "Epoch 17/100\n",
            "11584/11584 [==============================] - 9s 761us/step - loss: 83091.7949 - val_loss: 80924.0425\n",
            "Epoch 18/100\n",
            "11584/11584 [==============================] - 9s 750us/step - loss: 82795.2442 - val_loss: 74819.8960\n",
            "Epoch 19/100\n",
            "11584/11584 [==============================] - 8s 717us/step - loss: 80191.6654 - val_loss: 74774.2368\n",
            "Epoch 20/100\n",
            "11584/11584 [==============================] - 8s 733us/step - loss: 80971.9628 - val_loss: 77354.0842\n",
            "Epoch 21/100\n",
            "11584/11584 [==============================] - 9s 772us/step - loss: 80018.6468 - val_loss: 76653.7170\n",
            "Epoch 22/100\n",
            "11584/11584 [==============================] - 8s 731us/step - loss: 80082.7262 - val_loss: 80601.4073\n",
            "Epoch 23/100\n",
            "11584/11584 [==============================] - 8s 720us/step - loss: 79437.9015 - val_loss: 83188.3122\n",
            "Epoch 24/100\n",
            "11584/11584 [==============================] - 9s 765us/step - loss: 78582.1442 - val_loss: 76699.2315\n",
            "Epoch 25/100\n",
            "11584/11584 [==============================] - 8s 732us/step - loss: 77862.3131 - val_loss: 71975.4227\n",
            "Epoch 26/100\n",
            "11584/11584 [==============================] - 8s 733us/step - loss: 78578.3076 - val_loss: 74660.6685\n",
            "Epoch 27/100\n",
            "11584/11584 [==============================] - 9s 745us/step - loss: 78050.5236 - val_loss: 79996.5862\n",
            "Epoch 28/100\n",
            "11584/11584 [==============================] - 8s 730us/step - loss: 77288.5981 - val_loss: 75962.0267\n",
            "Epoch 29/100\n",
            "11584/11584 [==============================] - 9s 744us/step - loss: 77582.3424 - val_loss: 82652.3779\n",
            "Epoch 30/100\n",
            "11584/11584 [==============================] - 9s 760us/step - loss: 76459.7817 - val_loss: 71946.5571\n",
            "Epoch 31/100\n",
            "11584/11584 [==============================] - 9s 740us/step - loss: 76404.3286 - val_loss: 72304.9090\n",
            "Epoch 32/100\n",
            "11584/11584 [==============================] - 9s 768us/step - loss: 75257.2018 - val_loss: 70545.2704\n",
            "Epoch 33/100\n",
            "11584/11584 [==============================] - 9s 738us/step - loss: 76247.2076 - val_loss: 82092.7368\n",
            "Epoch 34/100\n",
            "11584/11584 [==============================] - 9s 767us/step - loss: 74901.8861 - val_loss: 73041.4663\n",
            "Epoch 35/100\n",
            "11584/11584 [==============================] - 9s 743us/step - loss: 76483.1575 - val_loss: 69943.7603\n",
            "Epoch 36/100\n",
            "11584/11584 [==============================] - 9s 763us/step - loss: 74082.7210 - val_loss: 77280.9017\n",
            "Epoch 37/100\n",
            "11584/11584 [==============================] - 9s 741us/step - loss: 73979.2370 - val_loss: 71431.9535\n",
            "Epoch 38/100\n",
            "11584/11584 [==============================] - 9s 754us/step - loss: 75333.8257 - val_loss: 71386.1355\n",
            "Epoch 39/100\n",
            "11584/11584 [==============================] - 9s 745us/step - loss: 74058.4086 - val_loss: 69861.3724\n",
            "Epoch 40/100\n",
            "11584/11584 [==============================] - 9s 758us/step - loss: 73634.3213 - val_loss: 85836.4514\n",
            "Epoch 41/100\n",
            "11584/11584 [==============================] - 9s 744us/step - loss: 74680.6028 - val_loss: 76347.4730\n",
            "Epoch 42/100\n",
            "11584/11584 [==============================] - 9s 748us/step - loss: 73145.4493 - val_loss: 74598.1805\n",
            "Epoch 43/100\n",
            "11584/11584 [==============================] - 9s 751us/step - loss: 73449.2637 - val_loss: 67915.2594\n",
            "Epoch 44/100\n",
            "11584/11584 [==============================] - 9s 760us/step - loss: 72716.8840 - val_loss: 73849.9343\n",
            "Epoch 45/100\n",
            "11584/11584 [==============================] - 9s 753us/step - loss: 73146.5860 - val_loss: 70657.4972\n",
            "Epoch 46/100\n",
            "11584/11584 [==============================] - 9s 750us/step - loss: 73040.7644 - val_loss: 82169.6727\n",
            "Epoch 47/100\n",
            "11584/11584 [==============================] - 9s 746us/step - loss: 72898.9336 - val_loss: 75708.6798\n",
            "Epoch 48/100\n",
            "11584/11584 [==============================] - 9s 742us/step - loss: 73119.8068 - val_loss: 69409.5278\n",
            "Epoch 49/100\n",
            "11584/11584 [==============================] - 8s 731us/step - loss: 71473.3885 - val_loss: 70958.1840\n",
            "Epoch 50/100\n",
            "11584/11584 [==============================] - 9s 820us/step - loss: 71676.3923 - val_loss: 69128.1052\n",
            "Epoch 51/100\n",
            "11584/11584 [==============================] - 9s 792us/step - loss: 72414.6481 - val_loss: 71925.0174\n",
            "Epoch 52/100\n",
            "11584/11584 [==============================] - 8s 728us/step - loss: 72305.9743 - val_loss: 70619.8939\n",
            "Epoch 53/100\n",
            "11584/11584 [==============================] - 8s 716us/step - loss: 71582.9090 - val_loss: 68064.0684\n",
            "Epoch 54/100\n",
            "11584/11584 [==============================] - 8s 696us/step - loss: 71293.2032 - val_loss: 69916.5299\n",
            "Epoch 55/100\n",
            "11584/11584 [==============================] - 8s 677us/step - loss: 72747.8954 - val_loss: 74616.6817\n",
            "Epoch 56/100\n",
            "11584/11584 [==============================] - 9s 738us/step - loss: 71981.7462 - val_loss: 70158.7326\n",
            "Epoch 57/100\n",
            "11584/11584 [==============================] - 8s 732us/step - loss: 71697.3145 - val_loss: 69999.0422\n",
            "Epoch 58/100\n",
            "11584/11584 [==============================] - 8s 698us/step - loss: 71985.5423 - val_loss: 72052.1843\n",
            "Epoch 59/100\n",
            "11584/11584 [==============================] - 9s 736us/step - loss: 71974.6346 - val_loss: 69488.4088\n",
            "Epoch 60/100\n",
            "11584/11584 [==============================] - 9s 741us/step - loss: 70722.2763 - val_loss: 68445.6254\n",
            "Epoch 61/100\n",
            "11584/11584 [==============================] - 9s 739us/step - loss: 70496.6109 - val_loss: 70656.9341\n",
            "Epoch 62/100\n",
            "11584/11584 [==============================] - 8s 722us/step - loss: 70037.5491 - val_loss: 78395.2478\n",
            "Epoch 63/100\n",
            "11584/11584 [==============================] - 9s 762us/step - loss: 70828.9650 - val_loss: 69443.6712\n",
            "Epoch 64/100\n",
            "11584/11584 [==============================] - 9s 758us/step - loss: 71232.9412 - val_loss: 74735.7005\n",
            "Epoch 65/100\n",
            "11584/11584 [==============================] - 9s 782us/step - loss: 71542.5478 - val_loss: 67891.1058\n",
            "Epoch 66/100\n",
            "11584/11584 [==============================] - 9s 746us/step - loss: 70500.6694 - val_loss: 69699.5770\n",
            "Epoch 67/100\n",
            "11584/11584 [==============================] - 9s 758us/step - loss: 71176.2529 - val_loss: 68506.2508\n",
            "Epoch 68/100\n",
            "11584/11584 [==============================] - 9s 773us/step - loss: 70489.3397 - val_loss: 72978.1618\n",
            "Epoch 69/100\n",
            "11584/11584 [==============================] - 9s 759us/step - loss: 71173.7466 - val_loss: 71790.1734\n",
            "Epoch 70/100\n",
            "11584/11584 [==============================] - 9s 741us/step - loss: 70179.9297 - val_loss: 68075.0222\n",
            "Epoch 71/100\n",
            "11584/11584 [==============================] - 9s 751us/step - loss: 70655.3651 - val_loss: 79289.5664\n",
            "Epoch 72/100\n",
            "11584/11584 [==============================] - 8s 730us/step - loss: 71104.8645 - val_loss: 67075.4120\n",
            "Epoch 73/100\n",
            "11584/11584 [==============================] - 8s 731us/step - loss: 70007.7389 - val_loss: 77524.5724\n",
            "Epoch 74/100\n",
            "11584/11584 [==============================] - 9s 768us/step - loss: 69816.1467 - val_loss: 68220.1286\n",
            "Epoch 75/100\n",
            "11584/11584 [==============================] - 9s 769us/step - loss: 70064.2865 - val_loss: 70147.3560\n",
            "Epoch 76/100\n",
            "11584/11584 [==============================] - 8s 708us/step - loss: 69813.8633 - val_loss: 69723.9010\n",
            "Epoch 77/100\n",
            "11584/11584 [==============================] - 9s 757us/step - loss: 68942.4701 - val_loss: 69307.4913\n",
            "Epoch 78/100\n",
            "11584/11584 [==============================] - 9s 765us/step - loss: 70110.3990 - val_loss: 67452.5583\n",
            "Epoch 79/100\n",
            "11584/11584 [==============================] - 9s 734us/step - loss: 69646.1049 - val_loss: 67689.3307\n",
            "Epoch 80/100\n",
            "11584/11584 [==============================] - 9s 753us/step - loss: 69086.4285 - val_loss: 68776.4840\n",
            "Epoch 81/100\n",
            "11584/11584 [==============================] - 8s 719us/step - loss: 70065.0408 - val_loss: 69004.3086\n",
            "Epoch 82/100\n",
            "11584/11584 [==============================] - 8s 712us/step - loss: 70230.2891 - val_loss: 71035.1717\n",
            "Epoch 83/100\n",
            "11584/11584 [==============================] - 9s 770us/step - loss: 69227.9444 - val_loss: 70486.6660\n",
            "Epoch 84/100\n",
            "11584/11584 [==============================] - 9s 750us/step - loss: 69434.8190 - val_loss: 71400.3216\n",
            "Epoch 85/100\n",
            "11584/11584 [==============================] - 9s 747us/step - loss: 69332.3456 - val_loss: 70510.5829\n",
            "Epoch 86/100\n",
            "11584/11584 [==============================] - 10s 831us/step - loss: 68789.6572 - val_loss: 70774.0232\n",
            "Epoch 87/100\n",
            "11584/11584 [==============================] - 9s 794us/step - loss: 68719.1873 - val_loss: 71206.9093\n",
            "Epoch 88/100\n",
            "11584/11584 [==============================] - 9s 763us/step - loss: 69217.3191 - val_loss: 70587.8355\n",
            "Epoch 89/100\n",
            "11584/11584 [==============================] - 9s 751us/step - loss: 69195.8081 - val_loss: 68580.0367\n",
            "Epoch 90/100\n",
            "11584/11584 [==============================] - 9s 782us/step - loss: 68008.8549 - val_loss: 67427.2005\n",
            "Epoch 91/100\n",
            "11584/11584 [==============================] - 9s 741us/step - loss: 68735.8600 - val_loss: 76677.4978\n",
            "Epoch 92/100\n",
            "11584/11584 [==============================] - 9s 767us/step - loss: 68850.6068 - val_loss: 66866.5954\n",
            "Epoch 93/100\n",
            "11584/11584 [==============================] - 8s 728us/step - loss: 68075.1505 - val_loss: 70919.2127\n",
            "Epoch 94/100\n",
            "11584/11584 [==============================] - 9s 734us/step - loss: 68242.2215 - val_loss: 73084.8894\n",
            "Epoch 95/100\n",
            "11584/11584 [==============================] - 8s 729us/step - loss: 67436.6308 - val_loss: 73693.5623\n",
            "Epoch 96/100\n",
            "11584/11584 [==============================] - 9s 754us/step - loss: 67754.6240 - val_loss: 68359.2088\n",
            "Epoch 97/100\n",
            "11584/11584 [==============================] - 8s 711us/step - loss: 67905.6292 - val_loss: 74093.5205\n",
            "Epoch 98/100\n",
            "11584/11584 [==============================] - 9s 735us/step - loss: 68321.1458 - val_loss: 69399.7327\n",
            "Epoch 99/100\n",
            "11584/11584 [==============================] - 8s 721us/step - loss: 68602.9320 - val_loss: 80631.6298\n",
            "Epoch 100/100\n",
            "11584/11584 [==============================] - 8s 706us/step - loss: 67660.3032 - val_loss: 71349.2940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGVUPzNfNq7k",
        "colab_type": "code",
        "outputId": "87da19e7-795f-4270-b526-86156ea2f65b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3434
        }
      },
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Dense\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "maxscaler = MaxAbsScaler()\n",
        "\n",
        "x_train = maxscaler.fit_transform(x_train)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(8,activation = 'relu',input_shape=(19,)))\n",
        "model.add(Dense(16))\n",
        "model.add(Dense(32,activation = 'relu'))\n",
        "model.add(Dense(64))\n",
        "model.add(Dense(128,activation = 'relu'))\n",
        "model.add(Dense(256,activation = 'sigmoid'))\n",
        "model.add(Dense(128,activation = 'relu'))\n",
        "model.add(Dense(64))\n",
        "model.add(Dense(32))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'mean_absolute_error')\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "history = model.fit(x_train,y_train,batch_size = BATCH_SIZE,\n",
        "                    epochs = NUM_EPOCHS, validation_split = 0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11584 samples, validate on 2896 samples\n",
            "Epoch 1/100\n",
            "11584/11584 [==============================] - 3s 280us/step - loss: 258802.3604 - val_loss: 216485.5197\n",
            "Epoch 2/100\n",
            "11584/11584 [==============================] - 3s 241us/step - loss: 220276.9866 - val_loss: 216738.4285\n",
            "Epoch 3/100\n",
            "11584/11584 [==============================] - 3s 239us/step - loss: 183969.1322 - val_loss: 162375.0776\n",
            "Epoch 4/100\n",
            "11584/11584 [==============================] - 3s 240us/step - loss: 150572.7074 - val_loss: 145778.3256\n",
            "Epoch 5/100\n",
            "11584/11584 [==============================] - 3s 240us/step - loss: 145679.3810 - val_loss: 142332.0486\n",
            "Epoch 6/100\n",
            "11584/11584 [==============================] - 3s 239us/step - loss: 143106.4094 - val_loss: 141136.9633\n",
            "Epoch 7/100\n",
            "11584/11584 [==============================] - 3s 240us/step - loss: 142566.5012 - val_loss: 140905.5493\n",
            "Epoch 8/100\n",
            "11584/11584 [==============================] - 3s 242us/step - loss: 140343.7721 - val_loss: 136763.7927\n",
            "Epoch 9/100\n",
            "11584/11584 [==============================] - 3s 241us/step - loss: 139705.8012 - val_loss: 136191.9892\n",
            "Epoch 10/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 138176.9080 - val_loss: 137500.6111\n",
            "Epoch 11/100\n",
            "11584/11584 [==============================] - 3s 243us/step - loss: 137315.4103 - val_loss: 134420.5102\n",
            "Epoch 12/100\n",
            "11584/11584 [==============================] - 3s 243us/step - loss: 137791.4395 - val_loss: 142115.5558\n",
            "Epoch 13/100\n",
            "11584/11584 [==============================] - 3s 238us/step - loss: 136236.7327 - val_loss: 136224.7849\n",
            "Epoch 14/100\n",
            "11584/11584 [==============================] - 3s 240us/step - loss: 135438.6036 - val_loss: 133724.8973\n",
            "Epoch 15/100\n",
            "11584/11584 [==============================] - 3s 243us/step - loss: 135489.6734 - val_loss: 134956.5753\n",
            "Epoch 16/100\n",
            "11584/11584 [==============================] - 3s 243us/step - loss: 135645.3109 - val_loss: 133225.4912\n",
            "Epoch 17/100\n",
            "11584/11584 [==============================] - 3s 245us/step - loss: 134725.3739 - val_loss: 135123.0819\n",
            "Epoch 18/100\n",
            "11584/11584 [==============================] - 3s 243us/step - loss: 134537.8942 - val_loss: 132659.5109\n",
            "Epoch 19/100\n",
            "11584/11584 [==============================] - 3s 241us/step - loss: 134134.5028 - val_loss: 133754.3542\n",
            "Epoch 20/100\n",
            "11584/11584 [==============================] - 3s 239us/step - loss: 133731.3842 - val_loss: 138547.3310\n",
            "Epoch 21/100\n",
            "11584/11584 [==============================] - 3s 242us/step - loss: 133358.9086 - val_loss: 131959.1685\n",
            "Epoch 22/100\n",
            "11584/11584 [==============================] - 3s 241us/step - loss: 133888.2037 - val_loss: 135776.0190\n",
            "Epoch 23/100\n",
            "11584/11584 [==============================] - 3s 242us/step - loss: 133314.7069 - val_loss: 132326.1158\n",
            "Epoch 24/100\n",
            "11584/11584 [==============================] - 3s 241us/step - loss: 133047.8789 - val_loss: 137005.5833\n",
            "Epoch 25/100\n",
            "11584/11584 [==============================] - 3s 240us/step - loss: 132900.0862 - val_loss: 131677.2943\n",
            "Epoch 26/100\n",
            "11584/11584 [==============================] - 3s 241us/step - loss: 132280.4096 - val_loss: 132500.6163\n",
            "Epoch 27/100\n",
            "11584/11584 [==============================] - 3s 243us/step - loss: 132485.6434 - val_loss: 131377.0865\n",
            "Epoch 28/100\n",
            "11584/11584 [==============================] - 3s 238us/step - loss: 132396.1550 - val_loss: 131937.0095\n",
            "Epoch 29/100\n",
            "11584/11584 [==============================] - 3s 240us/step - loss: 132122.6618 - val_loss: 131807.4400\n",
            "Epoch 30/100\n",
            "11584/11584 [==============================] - 3s 240us/step - loss: 132505.8186 - val_loss: 131602.2716\n",
            "Epoch 31/100\n",
            "11584/11584 [==============================] - 3s 247us/step - loss: 131481.0569 - val_loss: 130272.8411\n",
            "Epoch 32/100\n",
            "11584/11584 [==============================] - 3s 243us/step - loss: 131545.2655 - val_loss: 130457.7119\n",
            "Epoch 33/100\n",
            "11584/11584 [==============================] - 3s 242us/step - loss: 132024.8755 - val_loss: 133260.0016\n",
            "Epoch 34/100\n",
            "11584/11584 [==============================] - 3s 256us/step - loss: 130475.4095 - val_loss: 137746.5773\n",
            "Epoch 35/100\n",
            "11584/11584 [==============================] - 3s 266us/step - loss: 130712.4283 - val_loss: 134361.9708\n",
            "Epoch 36/100\n",
            "11584/11584 [==============================] - 3s 295us/step - loss: 130964.9741 - val_loss: 131754.3432\n",
            "Epoch 37/100\n",
            "11584/11584 [==============================] - 3s 265us/step - loss: 130989.9938 - val_loss: 130456.9934\n",
            "Epoch 38/100\n",
            "11584/11584 [==============================] - 3s 266us/step - loss: 131000.4984 - val_loss: 129194.0661\n",
            "Epoch 39/100\n",
            "11584/11584 [==============================] - 3s 263us/step - loss: 130318.7982 - val_loss: 129996.5977\n",
            "Epoch 40/100\n",
            "11584/11584 [==============================] - 3s 246us/step - loss: 130741.9667 - val_loss: 133182.2710\n",
            "Epoch 41/100\n",
            "11584/11584 [==============================] - 3s 241us/step - loss: 129929.1358 - val_loss: 131650.6458\n",
            "Epoch 42/100\n",
            "11584/11584 [==============================] - 3s 240us/step - loss: 130390.6579 - val_loss: 130073.5780\n",
            "Epoch 43/100\n",
            "11584/11584 [==============================] - 3s 236us/step - loss: 129844.3776 - val_loss: 133532.7206\n",
            "Epoch 44/100\n",
            "11584/11584 [==============================] - 3s 239us/step - loss: 131089.0863 - val_loss: 136222.7474\n",
            "Epoch 45/100\n",
            "11584/11584 [==============================] - 3s 236us/step - loss: 130096.4126 - val_loss: 131646.1933\n",
            "Epoch 46/100\n",
            "11584/11584 [==============================] - 4s 349us/step - loss: 130012.2665 - val_loss: 141604.7315\n",
            "Epoch 47/100\n",
            "11584/11584 [==============================] - 5s 416us/step - loss: 130272.4953 - val_loss: 130737.5589\n",
            "Epoch 48/100\n",
            "11584/11584 [==============================] - 5s 427us/step - loss: 128967.0699 - val_loss: 130859.7740\n",
            "Epoch 49/100\n",
            "11584/11584 [==============================] - 5s 411us/step - loss: 129766.0221 - val_loss: 135512.4210\n",
            "Epoch 50/100\n",
            "11584/11584 [==============================] - 4s 382us/step - loss: 129822.7295 - val_loss: 131788.8355\n",
            "Epoch 51/100\n",
            "11584/11584 [==============================] - 5s 435us/step - loss: 128983.5152 - val_loss: 136966.8227\n",
            "Epoch 52/100\n",
            "11584/11584 [==============================] - 5s 421us/step - loss: 129158.5966 - val_loss: 129259.4966\n",
            "Epoch 53/100\n",
            "11584/11584 [==============================] - 5s 418us/step - loss: 129543.8524 - val_loss: 137813.6593\n",
            "Epoch 54/100\n",
            "11584/11584 [==============================] - 5s 422us/step - loss: 128790.1736 - val_loss: 129489.1136\n",
            "Epoch 55/100\n",
            "11584/11584 [==============================] - 5s 430us/step - loss: 129381.9340 - val_loss: 129177.1350\n",
            "Epoch 56/100\n",
            "11584/11584 [==============================] - 5s 426us/step - loss: 129607.2790 - val_loss: 141498.6874\n",
            "Epoch 57/100\n",
            "11584/11584 [==============================] - 5s 409us/step - loss: 128895.1133 - val_loss: 129999.3983\n",
            "Epoch 58/100\n",
            "11584/11584 [==============================] - 5s 410us/step - loss: 129197.2853 - val_loss: 130496.9816\n",
            "Epoch 59/100\n",
            "11584/11584 [==============================] - 5s 427us/step - loss: 128363.1496 - val_loss: 130853.8940\n",
            "Epoch 60/100\n",
            "11584/11584 [==============================] - 5s 401us/step - loss: 128618.7746 - val_loss: 128738.4704\n",
            "Epoch 61/100\n",
            "11584/11584 [==============================] - 5s 392us/step - loss: 128667.0117 - val_loss: 134778.0997\n",
            "Epoch 62/100\n",
            "11584/11584 [==============================] - 5s 418us/step - loss: 127919.1859 - val_loss: 131621.6739\n",
            "Epoch 63/100\n",
            "11584/11584 [==============================] - 5s 428us/step - loss: 128693.4105 - val_loss: 129432.6763\n",
            "Epoch 64/100\n",
            "11584/11584 [==============================] - 5s 412us/step - loss: 128186.8572 - val_loss: 128508.2179\n",
            "Epoch 65/100\n",
            "11584/11584 [==============================] - 5s 427us/step - loss: 127962.8378 - val_loss: 127865.6279\n",
            "Epoch 66/100\n",
            "11584/11584 [==============================] - 5s 449us/step - loss: 127740.9271 - val_loss: 128961.4849\n",
            "Epoch 67/100\n",
            "11584/11584 [==============================] - 5s 435us/step - loss: 127705.7039 - val_loss: 142541.0349\n",
            "Epoch 68/100\n",
            "11584/11584 [==============================] - 5s 444us/step - loss: 128085.9973 - val_loss: 130139.7353\n",
            "Epoch 69/100\n",
            "11584/11584 [==============================] - 5s 461us/step - loss: 127410.3191 - val_loss: 130011.4188\n",
            "Epoch 70/100\n",
            "11584/11584 [==============================] - 5s 424us/step - loss: 128550.8169 - val_loss: 129583.1072\n",
            "Epoch 71/100\n",
            "11584/11584 [==============================] - 5s 408us/step - loss: 127837.2221 - val_loss: 127492.8631\n",
            "Epoch 72/100\n",
            "11584/11584 [==============================] - 5s 417us/step - loss: 127922.3067 - val_loss: 135214.8390\n",
            "Epoch 73/100\n",
            "11584/11584 [==============================] - 5s 404us/step - loss: 127330.0747 - val_loss: 146230.9164\n",
            "Epoch 74/100\n",
            "11584/11584 [==============================] - 5s 444us/step - loss: 128325.1286 - val_loss: 128532.4828\n",
            "Epoch 75/100\n",
            "11584/11584 [==============================] - 5s 428us/step - loss: 127364.8298 - val_loss: 130564.0982\n",
            "Epoch 76/100\n",
            "11584/11584 [==============================] - 5s 439us/step - loss: 127257.6623 - val_loss: 132310.1029\n",
            "Epoch 77/100\n",
            "11584/11584 [==============================] - 5s 432us/step - loss: 127366.0862 - val_loss: 131316.4514\n",
            "Epoch 78/100\n",
            "11584/11584 [==============================] - 5s 433us/step - loss: 126766.2088 - val_loss: 131599.8614\n",
            "Epoch 79/100\n",
            "11584/11584 [==============================] - 5s 417us/step - loss: 127665.2942 - val_loss: 128610.4510\n",
            "Epoch 80/100\n",
            "11584/11584 [==============================] - 5s 419us/step - loss: 127115.3224 - val_loss: 127910.2724\n",
            "Epoch 81/100\n",
            "11584/11584 [==============================] - 5s 409us/step - loss: 126901.6021 - val_loss: 128469.6881\n",
            "Epoch 82/100\n",
            "11584/11584 [==============================] - 5s 431us/step - loss: 127350.1922 - val_loss: 130390.1538\n",
            "Epoch 83/100\n",
            "11584/11584 [==============================] - 5s 432us/step - loss: 127364.2279 - val_loss: 127491.2992\n",
            "Epoch 84/100\n",
            "11584/11584 [==============================] - 5s 418us/step - loss: 126468.4942 - val_loss: 131254.9645\n",
            "Epoch 85/100\n",
            "11584/11584 [==============================] - 5s 412us/step - loss: 126885.2990 - val_loss: 132096.7407\n",
            "Epoch 86/100\n",
            "11584/11584 [==============================] - 5s 442us/step - loss: 125946.1388 - val_loss: 130262.3422\n",
            "Epoch 87/100\n",
            "11584/11584 [==============================] - 5s 431us/step - loss: 127985.5781 - val_loss: 126374.1965\n",
            "Epoch 88/100\n",
            "11584/11584 [==============================] - 5s 429us/step - loss: 126218.3657 - val_loss: 127318.8918\n",
            "Epoch 89/100\n",
            "11584/11584 [==============================] - 5s 413us/step - loss: 126557.8405 - val_loss: 129748.4037\n",
            "Epoch 90/100\n",
            "11584/11584 [==============================] - 5s 422us/step - loss: 126610.1482 - val_loss: 126327.9924\n",
            "Epoch 91/100\n",
            "11584/11584 [==============================] - 5s 428us/step - loss: 126302.2226 - val_loss: 131833.3996\n",
            "Epoch 92/100\n",
            "11584/11584 [==============================] - 5s 432us/step - loss: 126729.4516 - val_loss: 127201.5384\n",
            "Epoch 93/100\n",
            "11584/11584 [==============================] - 5s 418us/step - loss: 125976.5305 - val_loss: 127966.4619\n",
            "Epoch 94/100\n",
            "11584/11584 [==============================] - 5s 426us/step - loss: 126384.5163 - val_loss: 126943.9195\n",
            "Epoch 95/100\n",
            "11584/11584 [==============================] - 5s 402us/step - loss: 126338.8312 - val_loss: 126064.0755\n",
            "Epoch 96/100\n",
            "11584/11584 [==============================] - 5s 408us/step - loss: 127095.6129 - val_loss: 126519.8176\n",
            "Epoch 97/100\n",
            "11584/11584 [==============================] - 5s 440us/step - loss: 125738.2536 - val_loss: 128150.2883\n",
            "Epoch 98/100\n",
            "11584/11584 [==============================] - 5s 445us/step - loss: 126083.3346 - val_loss: 127396.1364\n",
            "Epoch 99/100\n",
            "11584/11584 [==============================] - 5s 427us/step - loss: 125860.5883 - val_loss: 129248.6120\n",
            "Epoch 100/100\n",
            "11584/11584 [==============================] - 5s 451us/step - loss: 126442.3699 - val_loss: 128959.6219\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}